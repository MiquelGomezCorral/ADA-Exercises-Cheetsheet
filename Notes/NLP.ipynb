{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, nltk, gensim, sklearn\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove component of the pipeline if not needed\n",
    "<img src=\"spacy.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe('parser')\n",
    "nlp.remove_pipe('tagger')\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = list()\n",
    "\n",
    "for book_file in os.listdir(corpus_root):\n",
    "    if \".txt\" in book_file:\n",
    "        print(book_file)\n",
    "        with codecs.open(os.path.join(corpus_root,book_file),encoding=\"utf8\") as f:\n",
    "            books.append(f.read())\n",
    "\n",
    "# Remove extra lines\n",
    "books = [\" \".join(b.split()) for b in books]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " put in raw text, get a Spacy object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sentecne splitting\n",
    "\n",
    "- `doc.sents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent for sent in doc.sents]\n",
    "print('Sentence 1:',sentences[0],'\\n')\n",
    "print('Sentence 2:',sentences[1],'\\n')\n",
    "print('Sentence 3:',sentences[2],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization\n",
    "\n",
    "- `token.text for token in doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight.'\n",
    "\n",
    "doc = nlp(example)\n",
    "\n",
    "#strings are encoded to hashes\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(example,'\\n')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PoS Tagging\n",
    "\n",
    "- `[(token.text, token.pos_) for token in doc]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "print(example,'\\n')\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know what each means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spacy.explain('CCONJ'))\n",
    "print(spacy.explain('ADP'))\n",
    "print(spacy.explain('DET'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Name entity recognition: _a person, a country, a product or a book title_\n",
    "\n",
    "- `for ent in doc.ents`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built in entity types: <br>\n",
    "\n",
    "PERSON  People, including fictional.  <br>\n",
    "NORP\tNationalities or religious or political groups. <br>\n",
    "FAC\tBuildings, airports, highways, bridges, etc. <br>\n",
    "ORG\tCompanies, agencies, institutions, etc. <br>\n",
    "GPE\tCountries, cities, states. <br>\n",
    "LOC\tNon-GPE locations, mountain ranges, bodies of water. <br>\n",
    "PRODUCT\tObjects, vehicles, foods, etc. (Not services.) <br>\n",
    "EVENT\tNamed hurricanes, battles, wars, sports events, etc. <br>\n",
    "WORK_OF_ART\tTitles of books, songs, etc. <br>\n",
    "LAW\tNamed documents made into laws. <br>\n",
    "LANGUAGE\tAny named language. <br>\n",
    "DATE\tAbsolute or relative dates or periods. <br>\n",
    "TIME\tTimes smaller than a day. <br>\n",
    "PERCENT\tPercentage, including \"%\". <br>\n",
    "MONEY\tMonetary values, including unit. <br>\n",
    "QUANTITY\tMeasurements, as of weight or distance. <br>\n",
    "ORDINAL\t\"first\", \"second\", etc. <br>\n",
    "CARDINAL\tNumerals that do not fall under another type. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Remove stop words\n",
    "\n",
    "- `spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "print('First ten stop words:',list(spacy_stopwords)[:10])\n",
    "\n",
    "stop_words = [token.text for token in doc if token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Lemmatization\n",
    "\n",
    "- `[token.lemma_ for token in doc]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.text != token.lemma_:\n",
    "        print(token.text,'--->',token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Chunking: _shallow parsing_\n",
    "Noun chunks are \"base noun phrases\" – flat phrases that have a noun as their head -- a noun plus the words describing the noun – for example, \"the lavish green grass\" or \"the world’s largest tech fund\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `for chunk in doc.noun_chunks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Dependancy parsing\n",
    "The terms head and child describe the words connected by an arc in the dependency tree. The type of syntactic relation that connects the child to the head can be obtain through .dep_.\n",
    "\n",
    "- `token.head.text`\n",
    "- `[child for child in token.children]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"Token: '{token.text: <13}', Head: '{token.head.text: <10}', Children: {[child for child in token.children]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(example,'\\n')\n",
    "words = [token.text for token in doc]\n",
    "\n",
    "# five most common tokens\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common()\n",
    "\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "\n",
    "# five most common tokens\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common()\n",
    "\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Sentiment analysis\n",
    "The sentiment score consits of four values. Neutral, positive and negative sum to one. The final score is obtained by thresholding the compound value (e.g. +/-0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "vs = analyzer.polarity_scores(example)\n",
    "\n",
    "print('Negative sentiment:',vs['neg'])\n",
    "print('Neutral sentiment:',vs['neu'])\n",
    "print('Positive sentiment:',vs['pos'])\n",
    "print('Compound sentiment:',vs['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analisys of the sentences of a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sent = []\n",
    "[total_sent.append(analyzer.polarity_scores(sent.text)['compound']) for sent in doc.sents]\n",
    "plt.hist(total_sent,bins = 15)\n",
    "plt.xlim([-1,1])\n",
    "plt.ylim([0,8000])\n",
    "plt.xlabel('Compound sentiment')\n",
    "plt.ylabel('Number of sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Document classification: Can we detect paragraphs from Frankenstein?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load our corpus via NLTK this time\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "# ?PlaintextCorpusReader\n",
    "our_books = PlaintextCorpusReader(corpus_root, '.*.txt')\n",
    "print(our_books.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment the books into equally long chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "# Get the data\n",
    "book_id = {f:n for n,f in enumerate(our_books.fileids())} # dictionary of books\n",
    "\n",
    "chunks = list()\n",
    "chunk_class = list() # this list contains the original book of the chunk, for evaluation\n",
    "\n",
    "limit = 500 # how many chunks total\n",
    "size = 50 # how many sentences per chunk/page\n",
    "\n",
    "for f in our_books.fileids():\n",
    "    sentences = our_books.sents(f)\n",
    "    print(f,\":\")\n",
    "    print('Number of sentences:',len(sentences))\n",
    "    \n",
    "    # create chunks\n",
    "    chunks_of_sents = [x for x in get_chunks(sentences,size)] # this is a list of lists of sentences, which are a list of tokens\n",
    "    chs = list()\n",
    "    \n",
    "    # regroup so to have a list of chunks which are strings\n",
    "    for c in chunks_of_sents:\n",
    "        grouped_chunk = list()\n",
    "        for s in c:\n",
    "            grouped_chunk.extend(s)\n",
    "        chs.append(\" \".join(grouped_chunk))\n",
    "    print(\"Number of chunks:\",len(chs),'\\n')\n",
    "    \n",
    "    # filter to the limit, to have the same number of chunks per book\n",
    "    chunks.extend(chs[:limit])\n",
    "    chunk_class.extend([book_id[f] for _ in range(len(chs[:limit]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the chunks with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#initialize and specify minumum number of occurences to avoid untractable number of features\n",
    "#vectorizer = CountVectorizer(min_df = 2) if we want high frequency\n",
    "\n",
    "#create bag of words features\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "print('Number of samples:',X.toarray().shape[0])\n",
    "print('Number of features:',X.toarray().shape[1])\n",
    "\n",
    "#mask and convert to int Frankenstein\n",
    "Y = np.array(chunk_class) == 1\n",
    "Y = Y.astype(int)  \n",
    "\n",
    "#shuffle the data\n",
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "\n",
    "#split into training and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Topic detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk the text again (_same as before_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "processed_docs = list()\n",
    "for doc in nlp.pipe(chunks, n_process=5, batch_size=10):\n",
    "\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    ents = doc.ents  # Named entities\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list and keep only words of length 3 or more.\n",
    "    doc = [token for token in doc if token not in STOPWORDS and len(token) > 2]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "\n",
    "    processed_docs.append(doc)\n",
    "docs = processed_docs\n",
    "del processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add bigrams: _Only those that appear at least x times_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigrams too\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "# Add bigrams to docs (only ones that appear 15 times or more).\n",
    "bigram = Phrases(docs, min_count=15)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out frequent and rare words \n",
    "Create a dictionary representation of the documents, and filter out frequent and rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 5\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "#MmCorpus.serialize(\"models/corpus.mm\", corpus)\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of chunks: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "from gensim.models import LdaMulticore\n",
    "params = {'passes': 10, 'random_state': seed}\n",
    "base_models = dict()\n",
    "model = LdaMulticore(corpus=corpus, num_topics=4, id2word=dictionary, workers=6,\n",
    "                passes=params['passes'], random_state=params['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.show_topics(num_words=5))\n",
    "print(model.show_topic(1,20))\n",
    "\n",
    "sorted(model[corpus[0]],key=lambda x:x[1],reverse=True)\n",
    "data =  pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "pyLDAvis.display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment and acurazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment\n",
    "sent_to_cluster = list()\n",
    "for n,doc in enumerate(corpus):\n",
    "    if doc:\n",
    "        cluster = max(model[doc],key=lambda x:x[1])\n",
    "        sent_to_cluster.append(cluster[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "from collections import Counter\n",
    "for book, cluster in book_id.items():\n",
    "    assignments = list()\n",
    "    for real,given in zip(chunk_class,sent_to_cluster):\n",
    "        if real == cluster:\n",
    "            assignments.append(given)\n",
    "    most_common,num_most_common = Counter(assignments).most_common(1)[0] # 4, 6 times\n",
    "    print(book,\":\",most_common,\"-\",num_most_common)\n",
    "    print(\"Accuracy:\",num_most_common/limit)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Semantic analysis based on lexical categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "\n",
    "# PRE-BUILD CATEGORIES\n",
    "categories = list(lexicon.cats.keys())\n",
    "print(\"There are \", len(categories), \" categories\")\n",
    "for cat in categories[:15]:\n",
    "    print(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representative terms of a certain category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.cats[\"health\"][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analize a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(books[3])\n",
    "\n",
    "categories = [\"disappointment\", \"pain\", \"joy\", \"beauty\", \"affection\"]\n",
    "empath_features = lexicon.analyze(doc.text, categories=categories)\n",
    "empath_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empath_features = lexicon.analyze(doc.text, categories=categories, normalize = True)\n",
    "empath_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usecase: the evolution of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = range(0,len(doc.text),150000)\n",
    "love = []\n",
    "pain = []\n",
    "beauty = []\n",
    "affection = []\n",
    "\n",
    "\n",
    "for cnt,i in enumerate(bins[:-1]):\n",
    "    empath_features = lexicon.analyze(doc.text[bins[cnt]:bins[cnt+1]],\n",
    "                                      categories = [\"love\", \"pain\", \"joy\", \"beauty\", \"affection\"], normalize = True)\n",
    "    love.append(empath_features[\"love\"])\n",
    "    pain.append(empath_features[\"pain\"])\n",
    "    beauty.append(empath_features[\"beauty\"])\n",
    "    affection.append(empath_features[\"affection\"])\n",
    "    \n",
    "    \n",
    "plt.plot(love,label = \"love\")\n",
    "plt.plot(beauty, label = \"beauty\")\n",
    "plt.plot(affection, label = \"affection\")\n",
    "plt.plot(pain,label = \"pain\")\n",
    "\n",
    "plt.xlabel(\"progression in the book\")\n",
    "plt.ylabel(\"frequency of a category\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom cathegories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.create_category(\"healthy_food\", [\"healthy_food\",\"low_carb\",\"kale\",\"avocado\"], model=\"nytimes\")\n",
    "lexicon.create_category(\"healthy_food\", [\"healthy_food\",\"low_carb\",\"kale\",\"avocado\"], model=\"reddit\")\n",
    "lexicon.create_category(\"festive_food\", [\"festive_food\",\"turkey\",\"eggnog\"], model=\"nytimes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
