{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just get the dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_features = ['sex', 'age', 'sibsp', 'parch', 'fare']\n",
    "X = pd.get_dummies(titanic[titanic_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the previous data `AND REMOVE PREVOUS COLUMNS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    " 'animal_type',\n",
    " 'intake_condition',\n",
    " 'intake_type',\n",
    " 'sex_upon_intake',\n",
    " 'sex_upon_outcome',\n",
    " 'outcome_type'\n",
    "]\n",
    "data_dummies = pd.get_dummies(original_data, columns=categorical_features, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarize: `FIRST SPLIT THEN STD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_train_norm = StandardScaler().fit(X_train).transform(X_train)\n",
    "X_test_norm = StandardScaler().fit(X_test).transform(X_test)\n",
    "print(f\"MEAN Training after Norm: {X_train_norm.mean(axis=0)}\")\n",
    "print(f\"STD Training after Norm: {X_train_norm.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BEFORE\")\n",
    "print(f\"mean {X[to_std_columns].mean(axis=0)}\")\n",
    "print(f\"std {X[to_std_columns].std(axis=0)}\\n\\n\")\n",
    "\n",
    "to_std_columns = ['area', 'perimeter', 'compactness', 'lengthOfKernel',\n",
    "       'widthOfKernel', 'asymmetryCoefficient', 'lengthOfKernelGroove']\n",
    "# to_std_columns = X.columns\n",
    "\n",
    "X_std = X.copy()\n",
    "X_std[to_std_columns] = (X[to_std_columns] - X[to_std_columns].mean(axis=0)) / X[to_std_columns].std(axis=0)\n",
    "\n",
    "print(f\"AFTER\")\n",
    "print(f\"mean {X_std[to_std_columns].mean(axis=0)}\")\n",
    "print(f\"std {X_std[to_std_columns].std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider adding a column of 1s to add biases in some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm_const = sm.add_constant(X_train_norm)\n",
    "X_test_norm_const = sm.add_constant(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_df = toyota_dummies_df.drop(columns=\"Price\")\n",
    "y_df = toyota_dummies_df[[\"Price\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n",
    "print(f\"Training with {len(X_train)} samples\")\n",
    "print(f\"Testing with {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_spli(data, X_remove, y_col, proportion: float = 0.8, seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    N = len(data)\n",
    "    indices = list(range(N))\n",
    "    random.shuffle(indices)\n",
    "    split = int(N*proportion)\n",
    "\n",
    "    X = data.drop(columns=X_remove)\n",
    "    y = data[y_col]\n",
    "    X_train, y_train = X.iloc[:split], y.iloc[:split]\n",
    "    X_test , y_test  = X.iloc[split:], y.iloc[split:]\n",
    "    \n",
    "    print(f\"Training with {len(X_train)} samples\")\n",
    "    print(f\"Testing with {len(X_test)} samples\")\n",
    "    return X_train, y_train, X_test , y_test\n",
    "\n",
    "X_train, y_train, X_test , y_test = train_test_spli(data_dummies, X_remove=['adopted', 'outcome_type'], y_col=\"adopted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix + F_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_true, y_pred)\n",
    "# normalize\n",
    "conf_matrix = conf_matrix / len(y_true)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred, y_true, verbose: bool = True, plot: bool = False):\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))  # True Positives\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))  # True Negatives\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))  # False Positives\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))  # False Negatives\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{tp = :6d} | {fp = :6d}\")\n",
    "        print(f\"----------- |  -----------\")\n",
    "        print(f\"{fn = :6d} | {tn = :6d}\")\n",
    "        print(f\"{precision = :.4}\")\n",
    "        print(f\"{recall = :.4}\")\n",
    "        print(f\"{f_score = :.4}\")\n",
    "        \n",
    "    if plot:\n",
    "        confusion_matrix = np.array([[tp, fn],  \n",
    "                                    [fp, tn]]) \n",
    "\n",
    "        confusion_matrix = pd.DataFrame(confusion_matrix, \n",
    "                                    index=[\"Actual Positive\", \"Actual Negative\"], \n",
    "                                    columns=[\"Predicted Positive\", \"Predicted Negative\"])\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sn.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap='YlGnBu', cbar=False)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "thresholds = [i*0.1 for i in range(11)]\n",
    "# thresholds = np.linspace(0, 1, 100)\n",
    "for threshold in thresholds:\n",
    "    pred = distribution[:,1] > threshold\n",
    "    f_score, precision, recall = confusion_matrix(pred, y_test, verbose=False, plot=False)\n",
    "    f_scores += [f_score]\n",
    "    precisions += [precision]\n",
    "    recalls += [recall]\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.plot(thresholds, f_scores, marker=\"o\", label=\"f_scores\")\n",
    "plt.plot(thresholds, precisions, marker=\"o\", label=\"precisions\")\n",
    "plt.plot(thresholds, recalls, marker=\"o\", label=\"recalls\")\n",
    "# plt.plot(thresholds, f_scores, marker=\"\", label=\"f_scores\")\n",
    "# plt.plot(thresholds, precisions, marker=\"\", label=\"precisions\")\n",
    "# plt.plot(thresholds, recalls, marker=\"\", label=\"recalls\")\n",
    "\n",
    "plt.xlabel('Thresholds')\n",
    "plt.ylabel('Score')\n",
    "plt.title(f'Score vs Thresholds')\n",
    "# plt.ylim(0.9, 1.0)\n",
    "plt.xlim(thresholds[0] - 0.1, thresholds[-1] + .1)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_test, y_test_pred):\n",
    "    N = len(y_test)\n",
    "    correct = np.sum(y_test == y_test_pred)\n",
    "    return correct / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = array([1, 3, 5, ..., 1, 4, 2], dtype=int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R^2 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 score metric (wtf is this)\n",
    "\n",
    "- The R² score on test set is 0.812, which means that 81.2% of the variance in the target variable (Price) can be explained by the features in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R2 score on test dataset: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction\n",
    "\n",
    "- TSNE\n",
    "- PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_tsne = TSNE(n_components=2, init='random', learning_rate='auto', random_state=0).fit_transform(X)\n",
    "\n",
    "print(\"The features of the first sample are: %s\" % X_reduced_tsne[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_pca = PCA(n_components=2).fit(X).transform(X)\n",
    "\n",
    "print(\"The features of the first sample are: %s\" % X_reduced_pca[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(7,3), sharey=True)\n",
    "\n",
    "# Cluster the data in 3 groups\n",
    "labels = KMeans(n_clusters=3, random_state=0).fit_predict(X)\n",
    "\n",
    "# Plot the data reduced in 2d space with t-SNE\n",
    "axs[0].scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=labels, alpha=0.6)\n",
    "axs[0].set_title(\"t-SNE\")\n",
    "\n",
    "# Plot the data reduced in 2d space with PCA\n",
    "axs[1].scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=labels, alpha=0.6)\n",
    "axs[1].set_title(\"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you do not need a train/test split if you want to get the propensy score. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. Its sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `STANDARIZE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_column(names: list[str], df: pd.DataFrame):\n",
    "    for name in names:\n",
    "        df[name] = (df[name] - df[name].mean())/df[name].std()\n",
    "        \n",
    "standarize_column([\"age\", \"creatinine_phosphokinase\", \"ejection_fraction\", \"platelets\", \"serum_creatinine\", \"serum_sodium\"], df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Linear regression: Modelling time spent at the hospital\n",
    "\n",
    "- We will perform a regression analysis to model the number of days spent at the hospital, among the population of patients.\n",
    "\n",
    "\n",
    "- To get started with our model, we need two components:\n",
    "\n",
    "   1. The equation describing the model\n",
    "   2. The data\n",
    "   \n",
    "   \n",
    "- Equations are specified using patsy formula syntax. Important operators are:\n",
    "    1. `~` : Separates the left-hand side and right-hand side of a formula.\n",
    "    2. `+` : Creates a union of terms that are included in the model.\n",
    "    3. `:` : Interaction term.\n",
    "    3. `*` : `a * b` is short-hand for `a + b + a:b`, and is useful for the common case of wanting to include all interactions between a set of variables.\n",
    "    \n",
    "    \n",
    "- Intercepts are added by default.\n",
    "\n",
    "\n",
    "- Categorical variables can be included directly by adding a term C(a). More on that soon!\n",
    "\n",
    "\n",
    "- For (2), we can conveniently use pandas dataframe.\n",
    "\n",
    "### An example\n",
    "\n",
    "- Let's start with an example from our dataset. We are interested in two predictors: diabetes and high blood pressure. These are the two predictors that we want to use to fit the outcome, the number of days spent at the hospital, using a linear regression.\n",
    "\n",
    "- A model that achieves this is formulated as:\n",
    "        time ~ C(diabetes) + C(high_blood_pressure)\n",
    "        \n",
    "- We can create this model using smf.ols().\n",
    "\n",
    "- OLS stands for ordinary least squares linear regression.\n",
    "\n",
    "- The two components: the formula and the data are stated explicitly.\n",
    "\n",
    "- The terms in the formula are columns in pandas dataframe. Easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal least Squares "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add a column of ones so it can get a bias in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm_const = sm.add_constant(X_train_norm)\n",
    "X_test_norm_const = sm.add_constant(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model = sm.OLS(y_train, X_train_norm).fit()\n",
    "print(logistic_regression_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula='time ~ C(diabetes) + C(high_blood_pressure)', data=df)\n",
    "mod = smf.ols(formula='time ~ C(high_blood_pressure) * C(DEATH_EVENT,  Treatment(reference=0)) + C(diabetes)', data=df)\n",
    "\n",
    "res = mod.fit()\n",
    "print(res.summary())\n",
    "# Confident intervals\n",
    "res.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full process keeping the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_ext_norm = pd.DataFrame(scaler.fit_transform(X_train_ext), columns=X_train_ext.columns, index=X_train_ext.index)\n",
    "X_train_ext_norm_const = sm.add_constant(X_train_ext_norm)\n",
    "\n",
    "lr_model = sm.OLS(y_train, X_train_ext_norm_const).fit()\n",
    "print(lr_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** In the first model, `high_blood_pressure` is associated with an additive coefficient of around -25. Thus, in the model, whenever a patient has high blood pressure we deduce -25 days out of the prediction.\n",
    "In the second model, `high_blood_pressure` is associated with an multiplicative coefficient of around -0.22. This means that, in the model, whenever a patient has high blood pressure we multiply his or her outcome by $e^{-0.22} \\simeq 0.80$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "This coeficients show how the `log-odds` will change when unitary change of the variable happens\n",
    "- To go from p to odds: $\\text{odds} = \\frac{p}{1 - p}$\n",
    "- To go from p to log-odds: $\\text{log-odds} = \\ln\\left(\\frac{p}{1 - p}\\right)$\n",
    "- To go from log-odds to odds: $\\text{odds} = e^{\\text{log-odds}}$\n",
    "- To go from log-odds to p: $p = \\frac{1}{1 + e^{-\\text{log-odds}}}$\n",
    "- To go from odds to log-odds: $\\text{log-odds} = \\ln(\\text{odds})$\n",
    "- To go from odds to p: $p = \\frac{\\text{odds}}{1 + \\text{odds}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_to_log_odd(p):\n",
    "    return  np.log(p/(1-p))\n",
    "def log_odd_to_p(log_odd):\n",
    "    return 1/(1+np.exp(-log_odd))\n",
    "\n",
    "p = 0.1\n",
    "original_log_odd = p_to_log_odd(p)\n",
    "new_log_odd = original_log_odd + 0.66\n",
    "\n",
    "new_p = log_odd_to_p(new_log_odd)\n",
    "print(f\"{p = }\")\n",
    "print(f\"{original_log_odd = }\")\n",
    "print(f\"{new_log_odd = }\")\n",
    "print(f\"{new_p = }, increased by {p-new_p} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.logit(formula='DEATH_EVENT ~  age + creatinine_phosphokinase + ejection_fraction + \\\n",
    "                        platelets + serum_creatinine + serum_sodium + \\\n",
    "                        C(diabetes) + C(high_blood_pressure) +\\\n",
    "                        C(sex) + C(anaemia) + C(smoking) + C(high_blood_pressure)', data=df)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A lot of useful information is provided by default.\n",
    "\n",
    "`Coef` is the value assigned to each `variable` + the `bias`\n",
    "\n",
    "- The dependent variable : time (number of days at the hospital)\n",
    "- Method: The type of model that was fitted (OLS)\n",
    "- Nb observations: The number of datapoints (299 patients)\n",
    "- R2: The fraction of explained variance\n",
    "- A list of predictors\n",
    "- For each predictor: coefficient, standard error of the coefficients, p-value, 95% confidence intervals. \n",
    "  - We can see that one is a significant predictor if p < 0.5 (or very small numbers)\n",
    "- Warnings if there are numerical issues (hopefully not!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict / Propensity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"propensity_score\"] = res.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = lr_model.params.index\n",
    "coefficients = lr_model.params.values\n",
    "\n",
    "for coefficient, name in zip(coefficients, variables):\n",
    "    # if name != \"high_blood_pressure\": continue\n",
    "    # print(f\"A change in {name} of 1 unit, increases your dead probability by ~{coefficient: 0.4f}\")\n",
    "    print(f\"{name: <15}: {coefficient: 0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = res.params.index\n",
    "coefficients = res.params.values\n",
    "\n",
    "for coefficient, name in zip(coefficients, variables):\n",
    "    # if name != \"high_blood_pressure\": continue\n",
    "    print(f\"A change in {name} of 1 unit, increases your dead probability by ~{coefficient: 0.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature names\n",
    "variables = res.params.index\n",
    "\n",
    "# quantifying uncertainty!\n",
    "\n",
    "# coefficients\n",
    "coefficients = res.params.values\n",
    "\n",
    "# p-values\n",
    "p_values = res.pvalues\n",
    "\n",
    "# standard errors\n",
    "standard_errors = res.bse.values\n",
    "\n",
    "l1, l2, l3, l4 = zip(*sorted(zip(coefficients[1:], variables[1:], standard_errors[1:], p_values[1:])))\n",
    "# fancy plotting \n",
    "plt.errorbar(l1, np.array(range(len(l1))), xerr= 2*np.array(l3), linewidth = 1,\n",
    "             linestyle = 'none',marker = 'o',markersize= 3,\n",
    "             markerfacecolor = 'black',markeredgecolor = 'black', capsize= 5)\n",
    "\n",
    "plt.vlines(0,0, len(l1), linestyle = '--')\n",
    "\n",
    "plt.yticks(range(len(l2)),l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significancy Vriables (p < 0.05)\n",
    "\n",
    "GET THE NAME OF THE COLUMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = lr_model.pvalues\n",
    "\n",
    "significant_features = p_values[p_values < 0.05].index\n",
    "\n",
    "# IF a constant column is added\n",
    "significant_features = significant_features.drop('const')\n",
    "\n",
    "print(\"Significant features: \", significant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"propensity_score\"] = res.predict()\n",
    "treated = df[df[\"treat\"] == 1]\n",
    "control = df[df[\"treat\"] == 0]\n",
    "\n",
    "def similarity(p_1, p_2):\n",
    "    return 1-np.abs(p_1 - p_2)\n",
    "\n",
    "# THE HIGHER THE EASIER FOR THEM TO MATCH\n",
    "def get_similarity(control_row, treatment_row):\n",
    "    return(\n",
    "        1\n",
    "        - np.abs(control_row[\"propensity_score\"] - treatment_row[\"propensity_score\"])\n",
    "        - int(control_row[\"black\"]  !=  treatment_row[\"black\"])\n",
    "        - int(control_row[\"hispan\"]  !=  treatment_row[\"hispan\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Loop through all the pairs of instances\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "\n",
    "        # Calculate the similarity \n",
    "        similarity = similarity(\n",
    "            # control_row['Propensity_score'], treatment_row['Propensity_score']\n",
    "            control_row, treatment_row\n",
    "        )\n",
    "\n",
    "        # Add an edge between the two instances weighted by the similarity between them\n",
    "        G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "# Generate and return the maximum weight matching on the generated graph\n",
    "matching = nx.max_weight_matching(G)\n",
    "\n",
    "matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n",
    "\n",
    "balanced_df_1 = lalonde_data.iloc[matched]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "logistic.fit(X.values, y)\n",
    "\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men = np.array[[...],[...]]\n",
    "pred = logistic.predict([men])\n",
    "distribution = logistic.predict_proba([men])\n",
    "error = mean_squared_error(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test[\"sold_within_3_months\"].values, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(logistic, X, y, cv=10, method=\"predict_proba\")\n",
    "precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\").mean()\n",
    "recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities with a cross validationn\n",
    "y_pred = cross_val_predict(logistic, X, y, cv=10, method=\"predict_proba\")\n",
    "# Compute the False Positive Rate and True Positive Rate\n",
    "fpr, tpr, _ = roc_curve(y, y_pred[:, 1])\n",
    "# Compute the area under the fpt-tpf curve\n",
    "auc_score = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold optimization (Linear and binary search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "distribution = logistic.predict_proba(X_test_norm_const)\n",
    "thresholds = [i*0.001 for i in range(1001)]\n",
    "\n",
    "best_f_score = -float(\"inf\")\n",
    "best_thres = None\n",
    "for thres in thresholds:\n",
    "    pred = distribution[:,0] < thres\n",
    "    score =  f_score(y_test[\"sold_within_3_months\"].values, pred)\n",
    "    print(f\"  · {score = :0.4}\")\n",
    "    if score > best_f_score:\n",
    "        best_f_score = score\n",
    "        best_thres = thres\n",
    "        \n",
    "print(f\"Best Threshold {best_thres} with {best_f_score = :0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = logistic.predict_proba(X_test_norm_const)\n",
    "\n",
    "def binary_threshold_search(distribution, max_iter: int = 10):\n",
    "    ini, end = 0, 1\n",
    "    mid = (ini + end)/2\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        pred = distribution[:,0] < end\n",
    "        end_score =  f_score(y_test[\"sold_within_3_months\"].values, pred)\n",
    "        pred = distribution[:,0] < mid\n",
    "        mid_score =  f_score(y_test[\"sold_within_3_months\"].values, pred)\n",
    "        if mid_score > end_score:\n",
    "            end = mid\n",
    "        else: \n",
    "            ini = mid\n",
    "        \n",
    "        mid = (ini + end)/2\n",
    "\n",
    "    return mid, mid_score\n",
    "        \n",
    "best_thres, best_f_score = binary_threshold_search(distribution)\n",
    "print(f\"Best Threshold {best_thres} with {best_f_score = :0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(3) #k\n",
    "model.fit(X, y_moons)\n",
    "Z = model.predict([X[0]])\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "kmean = KMeans(n_clusters=n_clusters, random_state=42).fit(X)\n",
    "# Plot the data by using the labels as color\n",
    "ax.scatter(X[:,0], X[:,1], c=kmean.labels_, alpha=0.6)\n",
    "\n",
    "for c in kmean.cluster_centers_:\n",
    "    ax.scatter(c[0], c[1], marker=\"+\", color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(4,4), sharey=True)\n",
    "\n",
    "# Plot the clusters with K = 3\n",
    "labels = KMeans(n_clusters=3, random_state=0).fit_predict(X)\n",
    "axs.scatter(X[:,0], X[:,1], c=labels, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CLUSTERS = 2\n",
    "MAX_CLUSTERS = 10\n",
    "\n",
    "# Compute number of row and columns\n",
    "COLUMNS = 3\n",
    "ROWS = math.ceil((MAX_CLUSTERS-MIN_CLUSTERS)/COLUMNS)\n",
    "fig, axs = plt.subplots(ROWS, COLUMNS, figsize=(10,8), sharey=True, sharex=True)\n",
    "\n",
    "# Plot the clusters\n",
    "for n_clusters in range(MIN_CLUSTERS, MAX_CLUSTERS+1):\n",
    "    current_column = (n_clusters-MIN_CLUSTERS)%COLUMNS\n",
    "    current_row = (n_clusters-MIN_CLUSTERS)//COLUMNS\n",
    "    # Get the axis where to add the plot\n",
    "    ax = axs[current_row, current_column]\n",
    "    # Cluster the data with the current number of clusters\n",
    "    kmean = KMeans(n_clusters=n_clusters, random_state=42).fit(X)\n",
    "    # Plot the data by using the labels as color\n",
    "    ax.scatter(X[:,0], X[:,1], c=kmean.labels_, alpha=0.6)\n",
    "    ax.set_title(\"%s clusters\"%n_clusters)\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "    # Plot the centroids\n",
    "    for c in kmean.cluster_centers_:\n",
    "        ax.scatter(c[0], c[1], marker=\"+\", color=\"red\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette and Elbow \n",
    "Silhouette suggests that using n clusters is a fair tradeoff between the number of groups and their separation. The elbow method shows how the SSE reduction is less significant with more than n clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette\n",
    "\n",
    "- `silhouette_score(X, labels)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouettes = []\n",
    "\n",
    "# Try multiple k\n",
    "for k in range(2, 11):\n",
    "    # Cluster the data and assigne the labels\n",
    "    labels = KMeans(n_clusters=k, random_state=10).fit_predict(X)\n",
    "    # Get the Silhouette score\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouettes.append({\"k\": k, \"score\": score})\n",
    "    \n",
    "# Convert to dataframe\n",
    "silhouettes = pd.DataFrame(silhouettes)\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(silhouettes.k, silhouettes.score)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Silhouette score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method\n",
    "\n",
    "- `kmeans.inertia_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sse(X, start=2, end=11):\n",
    "    sse = []\n",
    "    for k in range(start, end):\n",
    "        # Assign the labels to the clusters\n",
    "        kmeans = KMeans(n_clusters=k, random_state=10).fit(X)\n",
    "        sse.append({\"k\": k, \"sse\": kmeans.inertia_})\n",
    "\n",
    "    sse = pd.DataFrame(sse)\n",
    "    # Plot the data\n",
    "    plt.plot(sse.k, sse.sse)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Sum of Squared Errors\")\n",
    "    \n",
    "plot_sse(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = axs[current_row, current_column]\n",
    "\n",
    "labels = DBSCAN(eps=eps).fit_predict(X_moons)\n",
    "\n",
    "ax.scatter(X_moons[:,0], X_moons[:,1], c=labels, alpha=0.6)\n",
    "ax.set_title(\"eps = {:.3f}\".format(eps))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of eps\n",
    "eps_list = np.linspace(0.05, 0.15, 14)\n",
    "\n",
    "# Compute number of row and columns\n",
    "COLUMNS = 7\n",
    "ROWS = math.ceil(len(eps_list)/COLUMNS)\n",
    "\n",
    "fig, axs = plt.subplots(ROWS, COLUMNS, figsize=(12, 4), sharey=True, sharex=True)\n",
    "\n",
    "for i in range(0, len(eps_list)):\n",
    "    eps = eps_list[i]\n",
    "    \n",
    "    current_column = i%COLUMNS\n",
    "    current_row = i//COLUMNS\n",
    "    \n",
    "    ax = axs[current_row, current_column]\n",
    "    labels = DBSCAN(eps=eps).fit_predict(X_moons)\n",
    "    ax.scatter(X_moons[:,0], X_moons[:,1], c=labels, alpha=0.6)\n",
    "    ax.set_title(\"eps = {:.3f}\".format(eps))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_trees = DecisionTreeClassifier(random_state=42)\n",
    "decision_trees = DecisionTreeClassifier(max_depth=8, random_state=42)\n",
    "\n",
    "decision_trees.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = decision_trees.predict(X_test)\n",
    "\n",
    "acc = accuracy_fn(y_test, y_test_pred)\n",
    "depth = decision_trees.get_depth()\n",
    "\n",
    "print(f\"Accuracy: {acc: 0.4f}, Depth: {depth}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(decision_trees, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = sklearn.tree.DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "tree_clf_trained = tree_clf.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(20, 10))  \n",
    "sklearn.tree.plot_tree(tree_clf_trained, feature_names=X_train.columns, filled=True, class_names=['Not sold', 'Sold'], fontsize=30)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = list(range(1, max_deph + 1))\n",
    "acc_hist = []\n",
    "for depth in depths:\n",
    "    decision_trees = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    decision_trees.fit(X_train, y_train)\n",
    "    y_test_pred_rf = decision_trees.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test[\"sold_within_3_months\"].values, y_test_pred_rf)\n",
    "    acc_hist += [acc]\n",
    "    print(f\"- Accuracy: {acc: 0.4f}, Depth: {depth}\")   \n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(depths, acc_hist, marker=\"o\")\n",
    "\n",
    "plt.xlabel('Max depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Accuracy vs max depth')\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.xlim(depths[0] - 1, depths[-1] + 1)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
