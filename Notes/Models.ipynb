{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET DUMMIES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just get the dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_features = ['sex', 'age', 'sibsp', 'parch', 'fare']\n",
    "X = pd.get_dummies(titanic[titanic_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    " 'animal_type',\n",
    " 'intake_condition',\n",
    " 'intake_type',\n",
    " 'sex_upon_intake',\n",
    " 'sex_upon_outcome',\n",
    " 'outcome_type'\n",
    "]\n",
    "data_dummies = pd.get_dummies(original_data, columns=categorical_features, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STANDARIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_std_columns = [\"intake_year\", \"intake_number\", \"age_upon_intake_(years)\", \"time_in_shelter_days\", \"age_upon_outcome_(years)\"]\n",
    "\n",
    "data_dummies[to_std_columns] = (data_dummies[to_std_columns] - data_dummies[to_std_columns].mean(axis=0)) / data_dummies[to_std_columns].std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_spli(data, X_remove, y_col, proportion: float = 0.8, seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    N = len(data)\n",
    "    indices = list(range(N))\n",
    "    random.shuffle(indices)\n",
    "    split = int(N*proportion)\n",
    "\n",
    "    X = data.drop(columns=X_remove)\n",
    "    y = data[y_col]\n",
    "    X_train, y_train = X.iloc[:split], y.iloc[:split]\n",
    "    X_test , y_test  = X.iloc[split:], y.iloc[split:]\n",
    "    \n",
    "    print(f\"Training with {len(X_train)} samples\")\n",
    "    print(f\"Testing with {len(X_test)} samples\")\n",
    "    return X_train, y_train, X_test , y_test\n",
    "\n",
    "X_train, y_train, X_test , y_test = train_test_spli(data_dummies, X_remove=['adopted', 'outcome_type'], y_col=\"adopted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFUSION MATRIX + FSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred, y_true, verbose: bool = True, plot: bool = False):\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))  # True Positives\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))  # True Negatives\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))  # False Positives\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))  # False Negatives\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{tp = :6d} | {fp = :6d}\")\n",
    "        print(f\"----------- |  -----------\")\n",
    "        print(f\"{fn = :6d} | {tn = :6d}\")\n",
    "        print(f\"{precision = :.4}\")\n",
    "        print(f\"{recall = :.4}\")\n",
    "        print(f\"{f_score = :.4}\")\n",
    "        \n",
    "    if plot:\n",
    "        confusion_matrix = np.array([[tp, fn],  \n",
    "                                    [fp, tn]]) \n",
    "\n",
    "        confusion_matrix = pd.DataFrame(confusion_matrix, \n",
    "                                    index=[\"Actual Positive\", \"Actual Negative\"], \n",
    "                                    columns=[\"Predicted Positive\", \"Predicted Negative\"])\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sn.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap='YlGnBu', cbar=False)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "thresholds = [i*0.1 for i in range(11)]\n",
    "# thresholds = np.linspace(0, 1, 100)\n",
    "for threshold in thresholds:\n",
    "    pred = distribution[:,1] > threshold\n",
    "    f_score, precision, recall = confusion_matrix(pred, y_test, verbose=False, plot=False)\n",
    "    f_scores += [f_score]\n",
    "    precisions += [precision]\n",
    "    recalls += [recall]\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.plot(thresholds, f_scores, marker=\"o\", label=\"f_scores\")\n",
    "plt.plot(thresholds, precisions, marker=\"o\", label=\"precisions\")\n",
    "plt.plot(thresholds, recalls, marker=\"o\", label=\"recalls\")\n",
    "# plt.plot(thresholds, f_scores, marker=\"\", label=\"f_scores\")\n",
    "# plt.plot(thresholds, precisions, marker=\"\", label=\"precisions\")\n",
    "# plt.plot(thresholds, recalls, marker=\"\", label=\"recalls\")\n",
    "\n",
    "plt.xlabel('Thresholds')\n",
    "plt.ylabel('Score')\n",
    "plt.title(f'Score vs Thresholds')\n",
    "# plt.ylim(0.9, 1.0)\n",
    "plt.xlim(thresholds[0] - 0.1, thresholds[-1] + .1)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you do not need a train/test split if you want to get the propensy score. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. Its sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `STANDARIZE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_column(names: list[str], df: pd.DataFrame):\n",
    "    for name in names:\n",
    "        df[name] = (df[name] - df[name].mean())/df[name].std()\n",
    "        \n",
    "standarize_column([\"age\", \"creatinine_phosphokinase\", \"ejection_fraction\", \"platelets\", \"serum_creatinine\", \"serum_sodium\"], df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Linear regression: Modelling time spent at the hospital\n",
    "\n",
    "- We will perform a regression analysis to model the number of days spent at the hospital, among the population of patients.\n",
    "\n",
    "\n",
    "- To get started with our model, we need two components:\n",
    "\n",
    "   1. The equation describing the model\n",
    "   2. The data\n",
    "   \n",
    "   \n",
    "- Equations are specified using patsy formula syntax. Important operators are:\n",
    "    1. `~` : Separates the left-hand side and right-hand side of a formula.\n",
    "    2. `+` : Creates a union of terms that are included in the model.\n",
    "    3. `:` : Interaction term.\n",
    "    3. `*` : `a * b` is short-hand for `a + b + a:b`, and is useful for the common case of wanting to include all interactions between a set of variables.\n",
    "    \n",
    "    \n",
    "- Intercepts are added by default.\n",
    "\n",
    "\n",
    "- Categorical variables can be included directly by adding a term C(a). More on that soon!\n",
    "\n",
    "\n",
    "- For (2), we can conveniently use pandas dataframe.\n",
    "\n",
    "### An example\n",
    "\n",
    "- Let's start with an example from our dataset. We are interested in two predictors: diabetes and high blood pressure. These are the two predictors that we want to use to fit the outcome, the number of days spent at the hospital, using a linear regression.\n",
    "\n",
    "- A model that achieves this is formulated as:\n",
    "        time ~ C(diabetes) + C(high_blood_pressure)\n",
    "        \n",
    "- We can create this model using smf.ols().\n",
    "\n",
    "- OLS stands for ordinary least squares linear regression.\n",
    "\n",
    "- The two components: the formula and the data are stated explicitly.\n",
    "\n",
    "- The terms in the formula are columns in pandas dataframe. Easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ordinal least Squares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula='time ~ C(diabetes) + C(high_blood_pressure)', data=df)\n",
    "mod = smf.ols(formula='time ~ C(high_blood_pressure) * C(DEATH_EVENT,  Treatment(reference=0)) + C(diabetes)', data=df)\n",
    "\n",
    "res = mod.fit()\n",
    "print(res.summary())\n",
    "# Confident intervals\n",
    "res.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** In the first model, `high_blood_pressure` is associated with an additive coefficient of around -25. Thus, in the model, whenever a patient has high blood pressure we deduce -25 days out of the prediction.\n",
    "In the second model, `high_blood_pressure` is associated with an multiplicative coefficient of around -0.22. This means that, in the model, whenever a patient has high blood pressure we multiply his or her outcome by $e^{-0.22} \\simeq 0.80$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "This coeficients show how the `log-odds` will change when unitary change of the variable happens\n",
    "- To go from p to odds: $\\text{odds} = \\frac{p}{1 - p}$\n",
    "- To go from p to log-odds: $\\text{log-odds} = \\ln\\left(\\frac{p}{1 - p}\\right)$\n",
    "- To go from log-odds to odds: $\\text{odds} = e^{\\text{log-odds}}$\n",
    "- To go from log-odds to p: $p = \\frac{1}{1 + e^{-\\text{log-odds}}}$\n",
    "- To go from odds to log-odds: $\\text{log-odds} = \\ln(\\text{odds})$\n",
    "- To go from odds to p: $p = \\frac{\\text{odds}}{1 + \\text{odds}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_to_log_odd(p):\n",
    "    return  np.log(p/(1-p))\n",
    "def log_odd_to_p(log_odd):\n",
    "    return 1/(1+np.exp(-log_odd))\n",
    "\n",
    "p = 0.1\n",
    "original_log_odd = p_to_log_odd(p)\n",
    "new_log_odd = original_log_odd + 0.66\n",
    "\n",
    "new_p = log_odd_to_p(new_log_odd)\n",
    "print(f\"{p = }\")\n",
    "print(f\"{original_log_odd = }\")\n",
    "print(f\"{new_log_odd = }\")\n",
    "print(f\"{new_p = }, increased by {p-new_p} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.logit(formula='DEATH_EVENT ~  age + creatinine_phosphokinase + ejection_fraction + \\\n",
    "                        platelets + serum_creatinine + serum_sodium + \\\n",
    "                        C(diabetes) + C(high_blood_pressure) +\\\n",
    "                        C(sex) + C(anaemia) + C(smoking) + C(high_blood_pressure)', data=df)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A lot of useful information is provided by default.\n",
    "\n",
    "`Coef` is the value assigned to each `variable` + the `bias`\n",
    "\n",
    "- The dependent variable : time (number of days at the hospital)\n",
    "- Method: The type of model that was fitted (OLS)\n",
    "- Nb observations: The number of datapoints (299 patients)\n",
    "- R2: The fraction of explained variance\n",
    "- A list of predictors\n",
    "- For each predictor: coefficient, standard error of the coefficients, p-value, 95% confidence intervals. \n",
    "  - We can see that one is a significant predictor if p < 0.5 (or very small numbers)\n",
    "- Warnings if there are numerical issues (hopefully not!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict / Propensity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"propensity_score\"] = res.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = res.params.index\n",
    "coefficients = res.params.values\n",
    "\n",
    "for coefficient, name in zip(coefficients, variables):\n",
    "    if name != \"high_blood_pressure\": continue\n",
    "    print(f\"A change in {name} of 1 unit, increases your dead probability by ~{coefficient: 0.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature names\n",
    "variables = res.params.index\n",
    "\n",
    "# quantifying uncertainty!\n",
    "\n",
    "# coefficients\n",
    "coefficients = res.params.values\n",
    "\n",
    "# p-values\n",
    "p_values = res.pvalues\n",
    "\n",
    "# standard errors\n",
    "standard_errors = res.bse.values\n",
    "\n",
    "l1, l2, l3, l4 = zip(*sorted(zip(coefficients[1:], variables[1:], standard_errors[1:], p_values[1:])))\n",
    "# fancy plotting \n",
    "plt.errorbar(l1, np.array(range(len(l1))), xerr= 2*np.array(l3), linewidth = 1,\n",
    "             linestyle = 'none',marker = 'o',markersize= 3,\n",
    "             markerfacecolor = 'black',markeredgecolor = 'black', capsize= 5)\n",
    "\n",
    "plt.vlines(0,0, len(l1), linestyle = '--')\n",
    "\n",
    "plt.yticks(range(len(l2)),l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"propensity_score\"] = res.predict()\n",
    "treated = df[df[\"treat\"] == 1]\n",
    "control = df[df[\"treat\"] == 0]\n",
    "\n",
    "def similarity(p_1, p_2):\n",
    "    return 1-np.abs(p_1 - p_2)\n",
    "\n",
    "# THE HIGHER THE EASIER FOR THEM TO MATCH\n",
    "def get_similarity(control_row, treatment_row):\n",
    "    return(\n",
    "        1\n",
    "        - np.abs(control_row[\"propensity_score\"] - treatment_row[\"propensity_score\"])\n",
    "        - int(control_row[\"black\"]  !=  treatment_row[\"black\"])\n",
    "        - int(control_row[\"hispan\"]  !=  treatment_row[\"hispan\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Loop through all the pairs of instances\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "\n",
    "        # Calculate the similarity \n",
    "        similarity = similarity(\n",
    "            # control_row['Propensity_score'], treatment_row['Propensity_score']\n",
    "            control_row, treatment_row\n",
    "        )\n",
    "\n",
    "        # Add an edge between the two instances weighted by the similarity between them\n",
    "        G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "# Generate and return the maximum weight matching on the generated graph\n",
    "matching = nx.max_weight_matching(G)\n",
    "\n",
    "matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n",
    "\n",
    "balanced_df_1 = lalonde_data.iloc[matched]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "logistic.fit(X.values, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "men = np.array[[...],[...]]\n",
    "pred = logistic.predict([men])\n",
    "distribution = logistic.predict_proba([men])\n",
    "error = mean_squared_error(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(logistic, X, y, cv=10, method=\"predict_proba\")\n",
    "precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\").mean()\n",
    "recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities with a cross validationn\n",
    "y_pred = cross_val_predict(logistic, X, y, cv=10, method=\"predict_proba\")\n",
    "# Compute the False Positive Rate and True Positive Rate\n",
    "fpr, tpr, _ = roc_curve(y, y_pred[:, 1])\n",
    "# Compute the area under the fpt-tpf curve\n",
    "auc_score = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(3) #k\n",
    "model.fit(X, y_moons)\n",
    "Z = model.predict([X[0]])\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
