{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph() # for a directed graph use nx.DiGraph()\n",
    "G.add_node(1)\n",
    "G.add_nodes_from(range(2,9))  # add multiple nodes at once\n",
    "\n",
    "# add edges \n",
    "G.add_edge(1,2)\n",
    "edges = [(2,3), (1,3), (4,1), (4,5), (5,6), (5,7), (6,7), (7,8), (6,8)]\n",
    "G.add_edges_from(edges)\n",
    "G.nodes()\n",
    "G.edges()\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas to nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges = pd.read_csv(data_folder + 'quakers_edgelist.csv')\n",
    "# edges.columns -> \"Source\", \"Target\"\n",
    "\n",
    "quakerG = nx.from_pandas_edgelist(edges, 'Source', 'Target', edge_attr=None, create_using= nx.Graph())\n",
    "describe_graph(quakerG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check by looking at the subgraphs induced by Alex and John\n",
    "subgraph_Alex = quakerG.subgraph(['Alexander Parker']+list(quakerG.neighbors('Alexander Parker')))\n",
    "subgraph_John = quakerG.subgraph(['John Crook']+list(quakerG.neighbors('John Crook')))\n",
    "\n",
    "nx.draw_spring(subgraph_Alex, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add atributes to nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add node attributes by passing dictionary of type name -> attribute\n",
    "nx.set_node_attributes(quakerG, nodes['Role'].to_dict(), 'Role' )\n",
    "nx.set_node_attributes(quakerG, nodes['Gender'].to_dict(), 'Gender' )\n",
    "nx.set_node_attributes(quakerG, nodes['Birthdate'].to_dict(), 'Birthdate' )\n",
    "nx.set_node_attributes(quakerG, nodes['Deathdate'].to_dict(), 'Deathdate' )\n",
    "nx.set_node_attributes(quakerG, nodes['Quaker'].to_dict(), 'Quaker' )\n",
    "\n",
    "quakerG.nodes['William Penn']\n",
    "-> {\n",
    "    'Role': 'Quaker leader and founder of Pennsylvania',\n",
    "    'Gender': 'male',\n",
    "    'Birthdate': 1644,\n",
    "    'Deathdate': 1718,\n",
    "    'Quaker': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(G, with_labels=True,  alpha = 0.8)\n",
    "nx.draw_circular(karateG, with_labels=True,  node_color='g', alpha = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for plotting the degree distribution of a Graph\n",
    "def plot_degree_distribution(G):\n",
    "    degrees = {}\n",
    "    for node in G.nodes():\n",
    "        degree = G.degree(node)\n",
    "        if degree not in degrees:\n",
    "            degrees[degree] = 0\n",
    "        degrees[degree] += 1\n",
    "    sorted_degree = sorted(degrees.items())\n",
    "    deg = [k for (k,v) in sorted_degree]\n",
    "    cnt = [v for (k,v) in sorted_degree]\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(deg, cnt, width=0.80, color='b')\n",
    "    plt.title(\"Degree Distribution\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"Degree\")\n",
    "    ax.set_xticks([d+0.05 for d in deg])\n",
    "    ax.set_xticklabels(deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing various graph properties\n",
    "def describe_graph(G):\n",
    "    print(G)\n",
    "    if nx.is_connected(G):\n",
    "        print(\"Avg. Shortest Path Length: %.4f\" %nx.average_shortest_path_length(G))\n",
    "        print(\"Diameter: %.4f\" %nx.diameter(G)) # Longest shortest path\n",
    "    else:\n",
    "        print(\"Graph is not connected\")\n",
    "        print(\"Diameter and Avg shortest path length are not defined!\")\n",
    "    print(\"Sparsity: %.4f\" %nx.density(G))  # #edges/#edges-complete-graph\n",
    "    # #closed-triplets(3*#triangles)/#all-triplets\n",
    "    print(\"Global clustering coefficient aka Transitivity: %.4f\" %nx.transitivity(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for visualizing the graph\n",
    "def visualize_graph(G, with_labels=True, k=None, alpha=1.0, node_shape='o'):\n",
    "    #nx.draw_spring(G, with_labels=with_labels, alpha = alpha)\n",
    "    pos = nx.spring_layout(G, k=k)\n",
    "    if with_labels:\n",
    "        lab = nx.draw_networkx_labels(G, pos, labels=dict([(n, n) for n in G.nodes()]))\n",
    "    ec = nx.draw_networkx_edges(G, pos, alpha=alpha)\n",
    "    nc = nx.draw_networkx_nodes(G, pos, nodelist=G.nodes(), node_color='g', node_shape=node_shape)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_graph(quakerG)\n",
    "visualize_graph(quakerG, False, k=0.2, alpha=0.4, node_shape='.')\n",
    "plot_degree_distribution(karateG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparcity\n",
    "\"Sparsity\" of a graph with $n$ nodes is defined as follows: \n",
    "\n",
    "$ L = \\frac{|E|}{|E_{max}|}$, where $E_{max} = \\frac{n * (n-1)}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(quakerG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.is_connected(quakerG))\n",
    "comp = list(nx.connected_components(quakerG))\n",
    "print('The graph contains', len(comp), 'connected components')\n",
    "\n",
    "largest_comp = max(comp, key=len)\n",
    "percentage_lcc = len(largest_comp)/quakerG.number_of_nodes() * 100\n",
    "print('The largest component has', len(largest_comp), 'nodes', 'accounting for %.2f'% percentage_lcc, '% of the nodes') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diameter and Shortest Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fell_whitehead_path = nx.shortest_path(quakerG, source=\"Margaret Fell\", target=\"George Whitehead\")\n",
    "print(\"Shortest path between Fell and Whitehead:\", fell_whitehead_path)\n",
    "\n",
    "# take the largest component and analyse its diameter = longest shortest-path\n",
    "lcc_quakerG = quakerG.subgraph(largest_comp)\n",
    "print(\"The diameter of the largest connected component is\", nx.diameter(lcc_quakerG))\n",
    "print(\"The avg shortest path length of the largest connected component is\", nx.average_shortest_path_length(lcc_quakerG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triadic Closure\n",
    "A *friend* of my *friend* is my *friend*   \n",
    "OR   \n",
    "*quaker_1* knows *quaker_2* and *quaker_2* knows *quaker_3*, how likely is that *quaker_1* and *quaker_3* know each other?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%.4f' %nx.transitivity(quakerG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employ a **local** measure called **clustering coefficient**, which quantifies for a node how close its neighbours are to being a clique (complete graph). Measured as the ratio of, the number of edges to the number of all possible edges, among the neighbors of a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar measure but for individual nodes called clustering coefficient\n",
    "print(nx.clustering(quakerG, ['Alexander Parker', 'John Crook']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEGREE\n",
    "degrees = dict(quakerG.degree(quakerG.nodes()))\n",
    "sorted_degree = sorted(degrees.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# And the top 5 most popular quakers are.. \n",
    "for quaker, degree in sorted_degree[:5]:\n",
    "    print(quaker, 'who is', quakerG.nodes[quaker]['Role'], 'knows', degree, 'people')\n",
    "    \n",
    "plot_degree_distribution(quakerG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about the Katz Centrality (the generalization over degree centrality)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(quakerG.degree(quakerG.nodes()))\n",
    "\n",
    "katz = nx.katz_centrality(quakerG)\n",
    "nx.set_node_attributes(quakerG, katz, 'katz')\n",
    "sorted_katz = sorted(katz.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# And the top 5 most popular quakers are.. \n",
    "for quaker, katzc in sorted_katz[:5]:\n",
    "    print(quaker, 'who is', quakerG.nodes[quaker]['Role'], 'has katz-centrality: %.3f' %katzc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betweeness centrality: the more shortest paths pass through a node, the more important it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute betweenness centrality\n",
    "betweenness = nx.betweenness_centrality(quakerG)\n",
    "# Assign the computed centrality values as a node-attribute in your network\n",
    "nx.set_node_attributes(quakerG, betweenness, 'betweenness')\n",
    "sorted_betweenness = sorted(betweenness.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "for quaker, bw in sorted_betweenness[:5]:\n",
    "    print(quaker, 'who is', quakerG.nodes[quaker]['Role'], 'has betweeness: %.3f' %bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homophily in quakers \n",
    "How likely is it that two quakers who have the same attribute are linked?\n",
    "\n",
    "Try to measure the similarity of connections in the graph with respect to a given attribute.   \n",
    "*Intuition: Like correlation, but translated to graphs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.attribute_assortativity_coefficient(quakerG, 'Gender'))\n",
    "print(nodes.groupby('Gender').size())\n",
    "\n",
    "nx.numeric_assortativity_coefficient(quakerG, 'Deathdate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communitie detection\n",
    "Community detection is a common class of methods applied to graphs. \n",
    "Two important algorithms:\n",
    "* **Girvan Newman**\n",
    "* **Louvain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Girvan Newman\n",
    "**Idea:** Edges possessing high betweeness centrality separate communities. Let's apply this on our toy sample graph (G) to get a better understanding of the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = girvan_newman(G)\n",
    "it = 0\n",
    "for communities in itertools.islice(comp, 4):\n",
    "    it +=1\n",
    "    print('Iteration', it)\n",
    "    print(tuple(sorted(c) for c in communities)) \n",
    "    \n",
    "visualize_graph(G,alpha=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The [Louvain method](https://en.wikipedia.org/wiki/Louvain_Modularity)\n",
    "\n",
    "Another clustering algorithm and has become a standard algorithm in the data scientist toolbox.   \n",
    "**Idea:** It proceeds the other way around: initially every node is considered as a community. The communities are traversed, and for each community it is tested whether by joining it to a neighboring community, we can obtain a better clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community_louvain.best_partition(quakerG)\n",
    "# add it as an attribute to the nodes\n",
    "for n in quakerG.nodes:\n",
    "    quakerG.nodes[n][\"louvain\"] = partition[n]\n",
    "    \n",
    "# plot it out\n",
    "pos = nx.spring_layout(quakerG,k=0.2)\n",
    "ec = nx.draw_networkx_edges(quakerG, pos, alpha=0.2)\n",
    "nc = nx.draw_networkx_nodes(quakerG, pos, nodelist=quakerG.nodes(), node_color=[quakerG.nodes[n][\"louvain\"] for n in quakerG.nodes], \n",
    "                            node_size=100, cmap=plt.cm.jet)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def similarity_metric(test, control):\n",
    "    dist = abs(test[\"Pred_Prob\"] - control[\"Pred_Prob\"])\n",
    "    if dist > 0.05:\n",
    "        return None\n",
    "    else:\n",
    "        return 1 - (dist)\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# G.add_nodes_from(range(len(data_df)))\n",
    "test_nodes = [f\"test_{i}\" for i in test_group.index]\n",
    "control_nodes = [f\"control_{i}\" for i in control_group.index]\n",
    "\n",
    "G.add_nodes_from(test_nodes, bipartite=0, group='test')\n",
    "G.add_nodes_from(control_nodes, bipartite=1, group='control')\n",
    "\n",
    "nx.set_node_attributes(G, data_df['Price'].to_dict(), 'Price' )\n",
    "nx.set_node_attributes(G, data_df['Discounted_Price'].to_dict(), 'Discounted_Price' )\n",
    "nx.set_node_attributes(G, data_df['Sold_within_3_months'].to_dict(), 'Sold_within_3_months' )       \n",
    "# nx.draw_spring(G, with_labels=True,  alpha = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_t, test in test_group.iterrows():\n",
    "    for idx_c, control in control_group.iterrows():\n",
    "        similarity = similarity_metric(test, control)\n",
    "        if similarity is not None:\n",
    "            G.add_edge(f\"test_{idx_t}\", f\"control_{idx_c}\", weight=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(G, with_labels=True,  alpha = 0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.matching import max_weight_matching\n",
    "\n",
    "# Compute the maximum weight matching\n",
    "matching = max_weight_matching(G, maxcardinality=True)\n",
    "\n",
    "# Print the matching\n",
    "print(f\"We have {len(matching)} successful matches!\")\n",
    "for u, v in matching:\n",
    "    print(f\"{u} - {v}, Weight: {G[u][v]['weight']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_df = data_df[data_df['Applied_Discount'] == 1]\n",
    "control_df = data_df[data_df['Applied_Discount'] == 0]\n",
    "\n",
    "print(f\"There are {treatment_df.shape[0]} samples in the treated group.\")\n",
    "print(f\"There are {control_df.shape[0]} samples in the control group.\")\n",
    "\n",
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    '''Calculate similarity for instances with given propensity scores'''\n",
    "    return 1 - np.abs(propensity_score1 - propensity_score2)\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "    for control_id, control_row in control_df.iterrows():\n",
    "        similarity = get_similarity(control_row['Pred_Prob'], treatment_row['Pred_Prob'])\n",
    "        if similarity>0.95:\n",
    "            G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "matching = nx.max_weight_matching(G, maxcardinality=True)\n",
    "print(f\"We have {len(matching)} successful matches!\")\n",
    "for u, v in matching:\n",
    "    print(f\"{u} - {v}, Weight: {G[u][v]['weight']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
